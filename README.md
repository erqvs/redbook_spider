# 小红书爬虫项目

## 项目简介

这是一个专门用于爬取小红书"安吉深蓝计划"相关内容的爬虫项目。项目包含两个主要功能模块：

1. **URL爬取模块** (`xiaohongshu_crawler.py`) - 自动搜索并收集相关帖子链接
2. **内容提取模块** (`extract_content.py`) - 从收集的链接中提取用户信息、标题和内容

## 功能特点

- 🔍 **智能搜索**：自动在小红书搜索"安吉深蓝计划"关键词
- 📊 **数据收集**：将搜索结果URL保存到Excel文件中
- 📝 **内容提取**：自动提取用户名称、帖子标题和详细内容
- 💾 **文件管理**：将提取的内容保存为独立的txt文件
- 🛡️ **反爬虫机制**：内置等待时间和重试机制，避免被平台封禁

## 项目结构

```
redbook_spyder/
├── xiaohongshu_crawler.py    # 主爬虫程序
├── extract_content.py         # 内容提取程序
├── xiaohongshu_urls.xlsx     # 收集的URL数据
├── final/                     # 提取的内容文件目录
│   ├── 用户1——标题1.txt
│   ├── 用户2——标题2.txt
│   └── ...
└── README.md                 # 项目说明文档
```

## 环境要求

- Python 3.7+
- DrissionPage 库
- pandas 库
- openpyxl 库

## 安装依赖

```bash
pip install DrissionPage pandas openpyxl
```

## 使用方法

### 1. 运行爬虫收集URL

```bash
python xiaohongshu_crawler.py
```

程序会自动：
- 打开浏览器访问小红书
- 等待30秒后搜索"安吉深蓝计划"
- 收集搜索结果中的URL
- 将URL保存到 `xiaohongshu_urls.xlsx` 文件中

### 2. 提取内容

```bash
python extract_content.py
```

程序会：
- 读取Excel文件中的URL
- 逐个访问每个页面
- 提取用户名称、标题和内容
- 保存为txt文件到 `final/` 目录

## 输出文件说明

### Excel文件格式
- **URL列**：小红书的帖子链接
- **时间戳列**：收集时间

### TXT文件格式
每个txt文件包含：
- 用户名称
- 帖子标题  
- 详细内容
- 原始URL

## 注意事项

⚠️ **重要提醒**：

1. **遵守平台规则**：请遵守小红书的用户协议和robots.txt规则
2. **合理使用频率**：程序内置了等待时间，避免过于频繁的请求
3. **仅供学习研究**：本项目仅供技术学习和研究使用
4. **数据使用规范**：提取的数据请勿用于商业用途
5. **网络环境**：确保网络连接稳定，避免中途断开

## 技术特点

- **多选择器策略**：使用多种CSS选择器确保数据提取的稳定性
- **重试机制**：内置重试逻辑，提高数据获取成功率
- **错误处理**：完善的异常处理，避免程序崩溃
- **文件管理**：自动清理文件名中的非法字符

## 更新日志

- **v1.0**：基础爬虫功能实现
- **v1.1**：增加内容提取模块
- **v1.2**：优化反爬虫机制和错误处理

## 免责声明

本项目仅用于技术学习和研究目的。使用者应当：

1. 遵守相关法律法规
2. 尊重网站的使用条款
3. 合理控制爬取频率
4. 不将数据用于商业用途

## 联系方式

如有问题或建议，请通过GitHub Issues联系。

---

**⚠️ 使用前请确保了解并遵守相关法律法规和平台规则！** 